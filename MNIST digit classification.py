# -*- coding: utf-8 -*-
"""
Created on Tue Nov 11 14:47:53 2025

@author: Ismail
"""


# ==============================================================
# OPTIMAL MNIST DIGIT CLASSIFIER - 10 EPOCHS (FIXED)
# ==============================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üöÄ Using device: {device}")

# ==============================================================
# OPTIMAL CNN ARCHITECTURE FOR MNIST (FIXED)
# ==============================================================

class OptimalMNISTNet(nn.Module):
    def __init__(self):
        super(OptimalMNISTNet, self).__init__()
        # Optimized for 28x28 images
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.dropout1 = nn.Dropout(0.25)  # Fixed: Use Dropout instead of Dropout2d
        self.dropout2 = nn.Dropout(0.5)   # Fixed: Use Dropout instead of Dropout2d
        self.fc1 = nn.Linear(64 * 3 * 3, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        # Input: 1x28x28
        x = F.relu(self.conv1(x))  # 32x28x28
        x = F.max_pool2d(x, 2)     # 32x14x14
        x = F.relu(self.conv2(x))  # 64x14x14
        x = F.max_pool2d(x, 2)     # 64x7x7
        x = F.relu(self.conv3(x))  # 64x7x7
        x = F.max_pool2d(x, 2)     # 64x3x3
        x = x.view(-1, 64 * 3 * 3) # Flatten
        x = self.dropout1(x)       # Fixed: Regular dropout after flattening
        x = F.relu(self.fc1(x))
        x = self.dropout2(x)       # Fixed: Regular dropout for fully connected
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

# ==============================================================
# TRAINING FUNCTION - 10 EPOCHS
# ==============================================================

def train_optimal_mnist():
    print("üì• Loading MNIST dataset...")
    
    # Enhanced data augmentation for training
    train_transform = transforms.Compose([
        transforms.RandomRotation(10),
        transforms.RandomAffine(0, translate=(0.1, 0.1)),
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    # Simple transform for testing
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    # Load datasets
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=train_transform)
    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=test_transform)
    
    # Data loaders
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)
    
    print(f"‚úÖ Dataset loaded: {len(train_dataset)} training, {len(test_dataset)} test images")
    
    # Create model
    model = OptimalMNISTNet().to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = StepLR(optimizer, step_size=4, gamma=0.5)
    
    print(f"üß† Model created with {sum(p.numel() for p in model.parameters()):,} parameters")
    
    # Training loop - 10 EPOCHS
    best_accuracy = 0
    print(f"\nüöÄ Starting training for 10 epochs...")
    print("=" * 60)
    
    for epoch in range(1, 11):  # 10 epochs total
        # Training phase
        model.train()
        train_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = F.nll_loss(output, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
        
        # Testing phase
        model.eval()
        test_loss = 0
        test_correct = 0
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                test_loss += F.nll_loss(output, target, reduction='sum').item()
                pred = output.argmax(dim=1, keepdim=True)
                test_correct += pred.eq(target.view_as(pred)).sum().item()
        
        test_loss /= len(test_loader.dataset)
        test_accuracy = 100. * test_correct / len(test_loader.dataset)
        train_accuracy = 100. * correct / total
        
        # Update learning rate
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f'Epoch {epoch:2d}/10:')
        print(f'  Train Acc: {train_accuracy:6.2f}% | Test Acc: {test_accuracy:6.2f}%')
        print(f'  Test Loss: {test_loss:.4f} | LR: {current_lr:.6f}')
        
        # Save best model
        if test_accuracy > best_accuracy:
            best_accuracy = test_accuracy
            torch.save(model.state_dict(), 'best_mnist_model.pth')
            print(f'  üíæ New best model saved! ({test_accuracy:.2f}%)')
        
        print('-' * 50)
    
    return best_accuracy

# ==============================================================
# PREDICTION FUNCTION
# ==============================================================

def predict_digit(image_tensor, model):
    """Predict a single digit"""
    model.eval()
    with torch.no_grad():
        output = model(image_tensor.unsqueeze(0).to(device))
        pred = output.argmax(dim=1, keepdim=True)
        confidence = torch.exp(output).max().item()
        return pred.item(), confidence

# ==============================================================
# LOAD AND TEST FUNCTION
# ==============================================================

def test_saved_model():
    """Test the saved model"""
    model = OptimalMNISTNet().to(device)
    model.load_state_dict(torch.load('best_mnist_model.pth', map_location=device))
    model.eval()
    
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=test_transform)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)
    
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    
    accuracy = 100. * correct / len(test_loader.dataset)
    print(f"üß™ Loaded model test accuracy: {accuracy:.2f}%")
    return accuracy

# ==============================================================
# MAIN EXECUTION
# ==============================================================

if __name__ == '__main__':
    print("üéØ OPTIMAL MNIST DIGIT CLASSIFIER - 10 EPOCHS (FIXED)")
    print("=" * 50)
    
    # Train the model
    best_accuracy = train_optimal_mnist()
    
    print("\n" + "=" * 50)
    print("‚úÖ TRAINING COMPLETED!")
    print("=" * 50)
    print(f"üèÜ Best Test Accuracy: {best_accuracy:.2f}%")
    print(f"üíæ Best model saved as: best_mnist_model.pth")
    
    # Test the saved model
    print("\nüß™ Testing saved model...")
    final_accuracy = test_saved_model()
    
    print("\nüîÆ Model is ready for digit recognition!")
    print("Usage: digit, confidence = predict_digit(image_tensor, model)")
    print("=" * 50)